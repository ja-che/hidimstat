{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Support recovery on simulated data (2D)\n\nThis example shows the advantages of spatially relaxed inference when\ndealing with high-dimensional spatial data. To do so, we compare several\nstatistical methods that aim at recovering the support, i.e., predictive\nfeatures. Among those methods some leverage the spatial structure of the\ndata. For more details about the inference algorithms presented in this\nexample or about the generative process used to simulate the data,\nplease refer to Chevalier et al. (2021) [1]_.\n\nThis example corresponds to the experiment described in details in\nChevalier et al. (2021) [1]_. Shortly, to simulate the data, we draw\n``n_samples`` i.i.d Gaussian vectors of size ``n_features`` and reshape them\ninto squares (edges are equal to ``n_features ** (1/2)``). Then, to introduce\nsome spatial structure, we apply a Gaussian filter that correlates features\nthat are nearby. The 2D data are then flattened into a design matrix ``X`` to\nrepresent it as a regression setting and to ease the computation of the\nsimulated target ``y`` (see below). Then, we construct the weight map ``w``\nwhich has the same shape as the 2D data, as it contains four predictive\nregions in every corner of the square. Similarly as for the construction\nof ``X``, the map ``w`` is finally flattened into a vector ``beta``. Lastly,\nto derive the target ``y``, we draw a white Gaussian noise ``epsilon`` and\nuse a linear generative model: ``y = X beta + epsilon``.\n\nThe results of this experiment show that the methods that leverage the spatial\nstructure of the data are relevant. More precisely, we show that clustered\ninference algorithms (e.g., CluDL) and ensembled clustered inference algorithms\n(e.g., EnCluDL) are more powerful than the standard inference methods (see also\nChevalier et al. (2021) [1]_). Indeed, when the number of features is much\ngreater than the number of samples, standard statistical methods are\nunlikely to recover the support. Then, the idea of clustered inference is to\ncompress the data without breaking the spatial structure, leading to a\ncompressed problem  close to the original problem. This leads to a\npowerful spatially relaxed inference. Indeed, thanks to the dimension reduction\nthe support recovery is feasible. However, due to the spatial compression,\nthere is a limited (and quantifiable) spatial uncertainty concerning the shape\nof the estimated support. Finally, by considering several choices of\nspatial compression, ensembled clustered inference algorithms reduce\nsignificantly the spatial uncertainty compared to clustered inference\nalgorithms which consider only one spatial compression.\n\n\n## References\n.. [1] Chevalier, J. A., Nguyen, T. B., Thirion, B., & Salmon, J. (2021).\n       Spatially relaxed inference on high-dimensional linear models.\n       arXiv preprint arXiv:2106.02590.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports needed for this script\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import FeatureAgglomeration\n\nfrom hidimstat.scenario import multivariate_simulation\nfrom hidimstat.stat_tools import zscore_from_pval, pval_from_cb\nfrom hidimstat.desparsified_lasso import desparsified_lasso\nfrom hidimstat.clustered_inference import clustered_inference\nfrom hidimstat.ensemble_clustered_inference import ensemble_clustered_inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specific plotting functions\nThe functions below are used to plot the results and illustrate the concept\nof spatial tolerance. If you are reading this example for the first time,\nyou can skip this section.\n\nThe following function builds a 2D map with four active regions that are\nenfolded by thin tolerance regions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def weight_map_2D_extended(shape, roi_size, delta):\n    '''Build weight map with visible tolerance region'''\n\n    roi_size_extended = roi_size + delta\n\n    w = np.zeros(shape + (5,))\n    w[0:roi_size, 0:roi_size, 0] = 0.5\n    w[-roi_size:, -roi_size:, 1] = 0.5\n    w[0:roi_size, -roi_size:, 2] = 0.5\n    w[-roi_size:, 0:roi_size, 3] = 0.5\n    w[0:roi_size_extended, 0:roi_size_extended, 0] += 0.5\n    w[-roi_size_extended:, -roi_size_extended:, 1] += 0.5\n    w[0:roi_size_extended, -roi_size_extended:, 2] += 0.5\n    w[-roi_size_extended:, 0:roi_size_extended, 3] += 0.5\n\n    for i in range(roi_size_extended):\n        for j in range(roi_size_extended):\n            if (i - roi_size) + (j - roi_size) >= delta:\n                w[i, j, 0] = 0\n                w[-i-1, -j-1, 1] = 0\n                w[i, -j-1, 2] = 0\n                w[-i-1, j, 3] = 0\n\n    beta_extended = w.sum(-1).ravel()\n\n    return beta_extended"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generate a plot that exhibits the true support and the estimated\nsupports for every method, we define the two following functions:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def add_one_subplot(ax, map, title):\n    '''Add one subplot into the summary plot'''\n\n    if map is not None:\n        im = ax.imshow(map)\n        im.set_clim(-1, 1)\n        ax.tick_params(\n            axis='both',\n            which='both',\n            bottom=False,\n            top=False,\n            left=False,\n            labelbottom=False,\n            labelleft=False)\n        ax.set_title(title)\n    else:\n        ax.axis('off')\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\n\ndef plot(maps, titles, save_fig=False):\n    '''Make a summary plot from estimated supports'''\n\n    fig, axes = plt.subplots(3, 2, figsize=(4, 6))\n\n    for i in range(3):\n        for j in range(2):\n            k = i * 2 + j\n            add_one_subplot(axes[i][j], maps[k], titles[k])\n\n    fig.tight_layout()\n\n    if save_fig:\n        figname = 'figures/simu_2D.png'\n        plt.savefig(figname)\n        print(f'Save figure to {figname}')\n\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating the data\n\nAfter setting the simulation parameters, we run the function that generates\nthe 2D scenario that we have briefly described in the first section of this\nexample.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# simulation parameters\nn_samples = 100\nshape = (40, 40)\nn_features = shape[1] * shape[0]\nroi_size = 4  # size of the edge of the four predictive regions\nsigma = 2.0  # noise standard deviation\nsmooth_X = 1.0  # level of spatial smoothing introduced by the Gaussian filter\n\n# generating the data\nX_init, y, beta, epsilon, _, _ = \\\n    multivariate_simulation(n_samples, shape, roi_size, sigma, smooth_X,\n                            seed=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing inference parameters\n\nThe choice of the number of clusters depends on several parameters, such as:\nthe structure of the data (a higher correlation between neighboring features\nenable a greater dimension reduction, i.e. a smaller number of clusters),\nthe number of samples (small datasets require more dimension reduction) and\nthe required spatial tolerance (small clusters lead to limited spatial\nuncertainty). Formally, \"spatial tolerance\" is defined by the largest\ndistance from the true support for which the occurence of a false discovery\nis not statistically controlled (c.f. `References`).\nTheoretically, the spatial tolerance ``delta`` is equal to the largest\ncluster diameter. However this choice is conservative, notably in the case\nof ensembled clustered inference. For these algorithms, we recommend to take\nthe average cluster radius. In this example, we choose ``n_clusters = 200``,\nleading to a theoretical spatial tolerance ``delta = 6``. However, it\nturns out that ``delta = 2``, the average cluster radius, would have been\nsufficient for ensembled clustered inference algorithms (see Results).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# hyper-parameters\nn_clusters = 200\n\n# inference parameters\nfwer_target = 0.1\ndelta = 6\n\n# computation parameter\nn_jobs = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing z-score thresholds for support estimation\n\nBelow, we translate the FWER target into z-score targets.\nTo compute the z-score targets we also take into account for the multiple\ntesting correction. To do so, we consider the Bonferroni correction.\nFor methods that do not reduce the feature space, the correction\nconsists in dividing the FWER target by the number of features.\nFor methods that group features into clusters, the correction\nconsists in dividing by the number of clusters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# computing the z-score thresholds for feature selection\ncorrection_no_cluster = 1. / n_features\ncorrection_cluster = 1. / n_clusters\nthr_c = zscore_from_pval((fwer_target / 2) * correction_cluster)\nthr_nc = zscore_from_pval((fwer_target / 2) * correction_no_cluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference with several algorithms\n\nFirst, we compute a reference map that exhibits the true support and\nthe theoretical tolerance region.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# compute true support with visible spatial tolerance\nbeta_extended = weight_map_2D_extended(shape, roi_size, delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we compute the support estimated by a high-dimensional statistical\ninfernece method that does not leverage the data structure. This method\nwas introduced by Javanmard, A. et al. (2014), Zhang, C. H. et al. (2014)\nand Van de Geer, S. et al.. (2014) (full references are available at\nhttps://ja-che.github.io/hidimstat/).\nand referred to as Desparsified Lasso.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# compute desparsified lasso\nbeta_hat, cb_min, cb_max = desparsified_lasso(X_init, y, n_jobs=n_jobs)\npval, pval_corr, one_minus_pval, one_minus_pval_corr = \\\n    pval_from_cb(cb_min, cb_max)\n\n# compute estimated support (first method)\nzscore = zscore_from_pval(pval, one_minus_pval)\nselected_dl = zscore > thr_nc  # use the \"no clustering threshold\"\n\n# compute estimated support (second method)\nselected_dl = np.logical_or(pval_corr < fwer_target / 2,\n                            one_minus_pval_corr < fwer_target / 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we compute the support estimated using a clustered inference algorithm\n(c.f. `References`) called Clustered Desparsified Lasso (CluDL) since it\nuses the Desparsified Lasso technique after clustering the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the FeatureAgglomeration object that performs the clustering.\n# This object is necessary to run the current algorithm and the following one.\nconnectivity = image.grid_to_graph(n_x=shape[0],\n                                   n_y=shape[1])\nward = FeatureAgglomeration(n_clusters=n_clusters,\n                            connectivity=connectivity,\n                            linkage='ward')\n\n# clustered desparsified lasso (CluDL)\nbeta_hat, pval, pval_corr, one_minus_pval, one_minus_pval_corr = \\\n    clustered_inference(X_init, y, ward, n_clusters)\n\n# compute estimated support (first method)\nzscore = zscore_from_pval(pval, one_minus_pval)\nselected_cdl = zscore > thr_c  # use the \"clustering threshold\"\n\n# compute estimated support (second method)\nselected_cdl = np.logical_or(pval_corr < fwer_target / 2,\n                             one_minus_pval_corr < fwer_target / 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we compute the support estimated by an ensembled clustered\ninference algorithm (c.f. `References`). This algorithm is called\nEnsemble of Clustered Desparsified Lasso (EnCluDL) since it runs several\nCluDL algorithms with different clustering choices. The different CluDL\nsolutions are then aggregated into one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ensemble of clustered desparsified lasso (EnCluDL)\nbeta_hat, pval, pval_corr, one_minus_pval, one_minus_pval_corr = \\\n    ensemble_clustered_inference(X_init, y, ward,\n                                 n_clusters, train_size=0.3)\n\n# compute estimated support\nselected_ecdl = np.logical_or(pval_corr < fwer_target / 2,\n                              one_minus_pval_corr < fwer_target / 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n\nNow we plot the true support, the theoretical tolerance regions and\nthe estimated supports for every method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "maps = []\ntitles = []\n\nmaps.append(np.reshape(beta, shape))\ntitles.append('True weights')\n\nmaps.append(np.reshape(beta_extended, shape))\ntitles.append('True weights \\nwith tolerance')\n\nmaps.append(np.reshape(selected_dl, shape))\ntitles.append('Desparsified Lasso')\n\nmaps.append(None)\ntitles.append(None)\n\nmaps.append(np.reshape(selected_cdl, shape))\ntitles.append('CluDL')\n\nmaps.append(np.reshape(selected_ecdl, shape))\ntitles.append('EnCluDL')\n\nplot(maps, titles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of the results\nAs argued in the first section of this example, the standard method that\ndo not compress the problem is not relevant as it dramatically lacks power.\nThe support estimated from CluDL provides a more reasonable solution\nsince we recover the four regions. However the shape of the estimated support\nis a bit rough.\nFinally, the solution provided by EnCluDL is more accurate since the shape\nof the estimated support is closer to the true support.\nAlso, one can note that the theoretical spatial tolerance is quite\nconservative. In practice, we argue that the statistical guarantees are valid\nfor a lower spatial tolerance thanks to the clustering randomization.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}